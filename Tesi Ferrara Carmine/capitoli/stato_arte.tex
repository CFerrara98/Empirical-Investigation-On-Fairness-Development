

\chapter{Stato dell'arte} %\label{1cap:spinta_laterale}
% [titolo ridotto se non ci dovesse stare] {titolo completo}

\definecolor{Gray}{gray}{0.9}

\section{Definizioni e Metriche di Software Fairness}
Dopo aver introdotto il background in cui il lavoro di tesi si si colloca, è senz'altro necessario introdurre quello che è l'aspetto principale che caratterizzerà il lavoro illustrato nei successivi capitoli, ovvero la Software Fairness e il codice eticamente corretto. Horkoff introduce alla fairness come un requisito non funzionale dei moduli di machine learning, per il quale  le attività di ricerca attuale sta investendo molto. In particolare si cerca di rendere gli algoritmi di machine learning più imparziali, non solo cercando di rimuovere features sensibili (come razza o sesso), ma cercando soprattutto di definire e implementare soluzioni AI-Intensive che tengano conto del livello di fairness richiesto dal dominio di applicazione\cite{NFRForML}.\\

Implementare un algoritmo eticamente corretto, significa definire formalmente cosa si intende per fairness, e cercare in conseguenza di misurare come ciò  viene implementato\cite{NFRForML}. Dato che concetto di Fairness è relativamente nuovo per la comunità scientifica, e soprattutto requisiti sociali e principi legali spesso ne evidenziano differenti caratteristiche,  sono emerse differenti definizioni del concetto e un numero speculare di metriche per misurarne il livello nei sistemi software\cite{evalFairClassification}. Prima di cercare di definire formalmente il concetto però è necessario cercare di capire in che maniera un sistema software mostra dei comportamenti scorretti e nello specifico, e come la comunità scientifica si pone rispetto ad esso.   \\

Recenti studi, inerenti gli aspetti di qualità e il successo del processo ingegneristico del software, hanno dimostrato che i sistemi software presentano un nuovo tipo di vulnerabilità, correlate appunti alla loro abilità di operare in maniera imparziale (fair) e priva di pregiudizi. Da dove nasce, quindi il problema della software fairness?  Come espresso in letteratura, il livello di fairness è strettamente correlato al concetto di Bias (pregiudizi algoritmici) che un sistema software ha al suo interno, questi comportamenti erronei possono emergere da vari aspetti, soprattutto appresi dai dati di addestramento (per quanto concerne le soluzioni AI-Intensive), ma anche per specifiche di requisiti incomplete, design povero, bug di implementazioni o interazioni errate tra componenti\cite{brun2018software}. Esistono differenti esempi interessanti di come fair bias nei sistemi software abbiano portato a spiacevoli inconvenienti, tra i più famosi vale la pena citare:

\begin{itemize}
    \item \emph{Software di Recruiting}: Gli specialisti di Machine Learning di Amazon, hanno reso noto che nel 2015, il loro nuovo tool di recruiting per lo sviluppo o altre posizioni tecniche, non offriva opportunità lavorative in maniera imparziale rispetto al sesso dei candidati, questo perché tale sistema era addestrato con dei dati di un periodi di 10 anni antecedente al 2015, per il quale la prevalenza degli impiegati tecnici dell'azienda è stata maschile, il gender e altri fattori (quali razza, oppure lo stesso linguaggio che gli impiegati tecnici maschili hanno adottato negli anni), sono stati classificati come feature sensibili che hanno addirittura portato Amazon, a chiudere il progetto, secondo quanto stabilito dalla loro versione ufficiale \cite{amazonrecruiting2018reuters};
    
    
    \item \emph{Healthcare}: Gli ospedali statunitensi per anni hanno utilizzato un software di predizione, per le cure mediche, che nel tempo è arrivato a \emph{preferire} i pazienti di razza bianca, rispetto a quelli di colore. Tale comportamento \emph{unfair} è da attribuire, purtroppo ad un bias di dati che considerava più \emph{conveniente} la popolazione bianca, dato che quest'ultima era soggetta a spese mediche maggiori rispetto a quelle di colore, quindi \emph{secondo il tool} preferendo tali individui su un campione di più di 200 milioni di abitanti, ci sarebbero stati maggiori guadagni per gli ospedali pubblici americani. Tale assunzione però si è convertita nella pratica nel considerare le persone di colore, maggiormente in salute rispetto quelle di carnagione chiara \cite{HealtCareBias};
    
    \item \emph{Crimine}: Nel 2014, si è osservato come un tool (denominato Correctional Offender Management Profiling for Alternative Sanctions, meglio conosciuto con l'acronimo di COMPAS), di predizione di responsabili di futuri crimini tra i più usati in Florida tra il 203 e il 2014, il cui funzionamento si basa sul confronto di analisi facciali partendo da  un dataset di immagini di fotografie di criminali condannati, avesse la tendenza a giudicare le persone di colore come più più predisposte a crimini violenti quali, la causa di questa tematica è ancora oggi molto discussa, ma si ritiene che la causa principale, sia nuovamente nei dati con cui il modello veniva addestrato, nel quale erano presenti feature sensibili, quali razza o sesso \cite{biasblack2016propublica};
    
    \item \emph{Traduzioni Automatiche}: Si è riscontrato come Google Traduttore, il più popolare motore di traduzione al mondo, mostrava un bias legato al sesso. in particolare, se si traduce dall'inglese al turco la frase "She is an engineer, He is a nurse", ne risulta un inversinone di soggetti: “He is an engineer, She is a nurse”, quasi ad indicare come le professioni tecniche, come quella di Ingegnere, in Turchia sia automaticamente associato al sesso maschile\cite{biasblack2016propublica};
    
    \item \emph{Generazione automatica di sottotitoli}: Su Youtube, se si seleziona la traduzione automatica, si osserva come effettivamente la traduzione del video risulti essere maggiormente accurata con voce maschile, rispetto a voce femminile, inoltre si osserva come che per lingue come l'inglese o l'arabo, ci siano delle differenze qualitative circa il risultato della traduzione, per alcuni specifici dialetti \cite{tatman-2017-gender}.
\end{itemize}

Dagli esempi citati si osserva, quindi, come ricercatori e ingegneri del software producano sempre di più software di qualità, in risposta alla necessità di prendere delle decisioni eticamente corrette, che riguardano sempre di più la vita umana\cite{biasInML}. Di conseguenza, si nota come un tool software o una soluzione AI-Intensive utilizzata su larga scala, non possa più presentare vulnerabilità che vadano ad inficiare il suo livello di Fairness. In risposta a questa nuova issiue ingegneristica, ne deriva in maniera naturale che è necessario come applicare definire formalmente il concetto di Software Fairness e conseguenti processi standard di misurazione.
 Negli ultimi anni questa problematica, ha attirato l'attenzione di ricercatori nell'ambito dell'intelligenza artificiale, l'ingegneria del software e la comunità legislativa, con più di venti differenti notazioni di Software Fairness proposte e ovviamente quello che ne deriva è che non è che non è possibile darne un'unica definizione specifica, ma è necessario specializzare il concetto in riferimento ad una specifica problematica analizzata \cite{FairnessDefinitionExplained}.  Ponendo l'attenzione al problema di Classificazione del Machine Learning, già definito nel precedente capitolo di background, è possibile dare numerose definizioni di Fairness per un generico classificatore ML, sulla base di alcuni strumenti ampiamente utilizzati nell'ambito dell'Intelligenza Artificiale, ovvero: Le misure statistiche di validazione, le misure di similarità e distanza e il Casual Reasoning \cite{FairnessDefinitionExplained}.
 
 \subsection{Definizioni di Fairness basate su Metriche Statistiche}
  Sahil Verma e Julia Rubin, introducono a questa tipologia di definizioni basate su metriche statistiche fornendo alcuni concetti di base \cite{FairnessDefinitionExplained}: 
 \begin{itemize}
     \item Attributi sensibili o protetti: attributo di un dato dataset, sul quale si sta effettuando classificazione, per il quale il generico modulo AI di classificazione potrebbe produrre discriminazioni (come ad esempio gender o razza);
     \item Diretta conseguenza degli attributi sensibili possono essere i Gruppi Protetti, gruppi di individui che assumono un valore dell'attributo sensibile che potrebbe essere soggetto a discriminazione;
     \item Il valore reale di classificazione: ovvero il valore (categoria di appartenenza) assegnato un individuo del dataset assume sulla base base delle sue feature di riferimento;
     \item La probabilità di predizione [0-1]: ovvero la probabilità condizionata che un individuo appartenga ad una data categoria, sulla base dei dati che lo caratterizzano, i classificatori, per stimare queste probabilità fanno riferimento in generale a tutte le feature del dataset, e in casi particolari possono anche riferirsi ad attributi sensibili, per particolari scelte di implementazione;
     \item La decisione predetta [0-1]: ovvero la categoria di appartenenza dell'individuo determinata dal classificatore, sulla base del dataset.
 \end{itemize}
 
 Le metodologie statistiche di validazione di un classificatore, in particolare per classificatori binari (con due singole categorie di classificazione, per convenzione una positiva e una negativa), basano la loro analisi sui differenti stati in cui è possibile trovare uno specifico individuo, ovvero:
 
 \begin{itemize}
     \item \emph{Reale Positivo - True Positive} (TP), se il valore reale di predizione e la decisione predetta dal corrispondono entrambi alla categoria "positiva";
     \item \emph{Reale Negativo - True Negative} (TN), se il valore reale di predizione e la decisione predetta corrispondono entrambi alla categoria "negativa";
     \item \emph{Falso Positivo - False Positive} (FP), se la decisione predetta corrisponde alla categoria "positiva", mentre il valore reale di predizione è la categoria "negativa";
    \item \emph{Falso Negativo - False Negative} (FN), se la decisione predetta corrisponde alla categoria "negativa", mentre il valore reale di predizione è la categoria "positiva".
 \end{itemize}, 
 
 Determinando questi valori per ciascuno degli individui del dataset, è possibile estrarre numerose metriche utili alla validazione di un generico classificatore binario, le più famose sono note in letteratura sono:
 
 \ldots la \emph{precision}:
\begin{equation*}
\frac{\sum{TP}}{\sum{TP + FP}}
\end{equation*}


 \ldots  e la \emph{recall}:
\begin{equation*}
\frac{\sum{TP}}{\sum{TP + FN}}
\end{equation*}

Tali formule, con altre metriche statistiche ampiamente riconosciute, sono spiegate nel dettaglio, nel paper  di riferimento\cite{FairnessDefinitionExplained}, vale la pena osservare che tali metriche possono essere generalizzate anche nel caso si stia validando un classificatore con N categorie.

Questa piccola introduzione ai concetti basilari di classificazione, permette quindi di comprendere meglio qualche definizione di Fairness di natura probabilistica.  tra le più utilizzate nell'ambito della classificazione, tra le più importanti e utilizzate, vale la pena citare\cite{FairnessDefinitionExplained}:

 
\begin{enumerate}
     \item \emph{Group Fairness - Statistical Parity}: Un classificatore, soddisfa questa definizione di fairness, se individui di un gruppo protetto (G1), definito per un dato attributo sensibile (g), possano essere assegnati dal classificatore alla categoria positiva ( che quindi il valore predetto d sia pari ad 1) , con la stessa probabilità degli individui del gruppo non protetto (G2), per il quale l'attributo sensibile assume valore non discriminante:
     
     \begin{equation*}
        P(d = 1| g = X1) = P(d = 1 | g = X2)
    \end{equation*}
     
     dove X1 e X2 sono rispettivamente il valore discriminate e non discriminate dell'attributo sensibile g;
    
     \item \emph{Predictive Parity - Outcome Test}: Un classificatore soddisfa questa definizione, se entrambi i gruppi, protetto e non protetto, hanno stessa probabilità di assumere valore reale di classificazione (Y), pari ad 1 (categoria positiva), data decisione predetta pari ad 1;
     \begin{equation*}
        P(Y = 1| d = 1 \& g = X1) = P( Y = 1 | d = 1 \& g = X2)
    \end{equation*}
    
    
     \item \emph{False Positive Error Rate Balance}: Un classificatore soddisfa questa definizione, se, la probabilità di essere classificati come appartenenti alla categoria positiva, (decisione predetta d = 1), pur appartenendo in realtà alla categoria negativa (valore reale di classificazione Y = 1), sia equivalente sia per individui appartenenti al gruppo protetto che per quelli appartenenti al gruppo non protetto.
     
     \begin{equation*}
       P(d = 1| Y = 0 \& g = X1) = P( d = 1 | Y = 0 \& g = X2)
    \end{equation*}
    
     
\end{enumerate}

- N.B. Le definizioni riportate prendono come dominio di applicazione i classificatori binari, con attributi sensibili con due possibili valori, ma è senz'altro possibile formalizzarle per casi generali - \\ \\
Queste ed altre definizioni statistiche del concetto di Software Fairness, vengono illustrate più in dettaglio nel paper di riferimento \cite{evalFairClassification}, anche in termini di metriche di validazione statistica quali Precision e Recall. Quello che però è importante sottolineare è l'enorme dinamicità di queste definizioni statistiche, come si nota come gli esempi riportati (come altre definizioni statistiche di Fairness presenti in letteratura), modellano concetti di probabilità differenti, ognuno avente un significato semantico ben preciso, e per avere una buona idea del livello statistico di Fairness di un classificatore sotto analisi, è necessario identificare bene quali metriche tenere in considerazione. Come è buona pratica di questi studi statistici può essere anche opportuno considerare e incrociare i risultati più metriche per formalizzare considerazioni più attendibili.

 \subsection{Definizioni di Fairness basate su Metriche di Similarità}
 
 Continuando questo excursus, nell'analisi delle definizioni di Software Fairness, nell'ambito dei classificatori ML, è possibile è possibile notare, come le definizioni statistiche, ignorino largamente tutti gli attributi di classificazione di un soggetto (sia X l'insieme degli attributi non sensibili di un generico individuo per cui si vuole effettuare classificazione), a meno di quelli sensibili G. Ad esempio, la Statistical Parity può determinare un classificatore come \emph{Fair}, senza tener conto delle evidenze dei valori sensibili (G), tralasciando il valore di tutte le altre features (X) del dataset. Quello che ne evince è che tali misure, per quanto di largo utilizzo, possono definire unfair un classificatore, senza però tenere conto di altri vincoli di implementazione\cite{FairnessDefinitionExplained}. Per venire incontro a questa problematica, il paper di riferimento, propone altre definizioni che non fanno utilizzo del concetto di probabilità, ma osservano appunto la similarità statistica tra i singoli individui, tenendo anche conto degli attributi sensibili\cite{FairnessDefinitionExplained}.\\
 
 \begin{enumerate}
     \item \emph{Casual Discrimination}: Un classificatore soddisfa questa definizione, se produce lo stesso risultato di classificazione (d - decisione predetta), per ogni coppia di individui con i medesimi attributi X (non sensibili): dati due individui m ed f, tale definizione è rispettata se rispetta la seguente implicazione;  
     
     
     \begin{equation*}
    X_m = X_f \ \&\& \ G_f != G_M \xrightarrow{} d_m = d_f
    \end{equation*} 

     \item \emph{Fairness Through Unawareness}: un classificatore soddisfa questa definizione, se per ogni coppia di individui, con lo stesso insieme di attributi non sensibili, si ottiene la stessa decisione predetta, in altre parole, nessun attributo sensibile è coinvolto nel processo di classificazione:dati due individui I1 e I2, il classificatore rispetta questa definizione se vale la seguente implicazione;  
     
         
     \begin{equation*}
        X_1 = X_2 \xrightarrow{} d_1 = d_2
    \end{equation*} 
 \end{enumerate}
 
 \subsection{Definizioni basate sul concetto di Casual Reasoning}
 Oltre le definizioni basate sulla statistica matematica, il paper di riferimento, riporta anche qualche definizione, basata sul concetto di Casual Reasoning: Processo di individuazione di relazioni causale (dipendenze cause/effetto tra attributi), molto utilizzato  per costruire Classificatori e altri algoritmi ML  \cite{FairnessDefinitionExplained}. L'output di un processo di Casual Reasoning per un classificatore ML, è solitamente un Casual Graph, un grafo aciclico, orientato, che connette le singole feature del dataset in relazione causa/effetto. Caratteristica interessante di un casual graph per un classificatore ML, è la presenza di un nodo terminale con solo archi entranti che rappresenta la decisione predetta dal classificatore (d). Facendo utilizzo di un Casual Graph è possibile identificare facilmente i percorsi di dipendenza (causa/effetto) che caratterizzano le scelte del classificatore corrispondente.
 
 Il paper definisce come componenti di un Casual Graph \cite{FairnessDefinitionExplained}, anche:
 \begin{itemize}
     \item I \emph{Proxy Attribute}: attributi di un casual graph utilizzati per derivarne altri, che spesso sono utilizzati per ottemperare a necessarie trasformazioni matematiche all'interno di del processo di classificazione;
     \item I \emph{Resolving Attribute}: attributi di un casual graph influenzati da  attributi di tipo sensibile (quindi interconnessi con i medesimi), in maniera non discriminatoria;
 \end{itemize}
 
 Anche per il Casual Reasoning, il paper riporta qualche definizione di Fairness, in questo caso direttamente individuabili dalla composizione strutturale di un Casual Graph, relativo al classificatore sotto analisi \cite{FairnessDefinitionExplained}.
 
 \begin{enumerate}
     \item \emph{Conterfactual Fairness}: un classificatore rispetta questa definizione, se nessun risultato predetto d non discende in alcun modo (tramite percorsi di casualità) da un attributo identificato come sensibile;
     \item \emph{No Unresolved Discrimination}: Un classificatore rispetta questa definizione, se nel Casual Graph di riferimento, non ci sono path da attributi sensibili ai risultati predetti, a meno che nel path non siano identificabili \textit{Resolving Attribute};
     \item \emph{No Proxy Discrimination}: Un classificatore rispetta questa definizione, se nel Casual Graph di riferimento, non ci sono path da attributi sensibili ai risultati predetti, "bloccati" da \textit{Proxy Attribute}, in altri termini, gli attributi sensibili non devono incidere nelle decisioni del classificatore per effetto di processi di trasformazione applicanti la strategia dei Proxy Attribute;
 \end{enumerate}






\newpage
\section{Software Fairness come tematica di ricerca}


\begin{longtable}{| p{.20\textwidth} | p{.35\textwidth} | p{.35\textwidth} |} 
\hline\textbf{\textit{Autori}} & \textbf{\textit{Obiettivi}} & \textbf{\textit{Soluzione}}\\
\hline
\endhead % all the lines above this will be repeated on every page
\multicolumn{3}{|l|}{\textbf{\textit{{* Paper}}}: Bias in Machine Learing Software: Why? How? What to Do? ~\cite{biasInML}} \\ 

\hline 
 Chakraborty et al.

& Identificare principali cause di fair bias in un datasets.

& Produzione dell'algoritmo Fair-Smoot per il Data Balancing di feature sensibili  

\\ \hline

\rowcolor{Gray}
\multicolumn{3}{|l|}{\textbf{\textit{* Paper}}: Ignorance and Prejudice in Software Fairness~\cite{Ignorance&Prejudice}} \\ \hline
\rowcolor{Gray}
M.Zhang e Mark Harmon         

& Bilanciare dati e features di un dataset, al fine di elaborare un modello efficiente di Machine Learning Fair Oriented~

& Analisi dei dati statistica, sulla base di differenti datasets, circa il bilanciamento ottimale di feature e campioni di dati al fine di ottenere livelli di fairness ottimali secondo opportune metriche 
\\ \hline


\multicolumn{3}{|l|}{\textbf{\textit{* Paper}}: Diversity Data  Selection under Fairness Constraint ~\cite{moumoulidou2020diverse}} \\ \hline

Moumoulidou at al.            

& Analizzare  e valutare la complessità  del problema  della data-diversity  per soluzioni ML-Intensive                                                     
& Dimostrazione formale dell'NP-Completezza  dell'approccio fair Max-Min Diversification e creazione di alcune approssimazioni dello stesso

\\ \hline
\rowcolor{Gray}
\multicolumn{3}{|l|}{\textbf{\textit{** Paper}}: Software Fairness~\cite{brun2018software}} 

\\ \hline
\rowcolor{Gray}
Yuriy Brun e Alexandra Meliou 

& Valutare il concetto di Software Fairness come un requisito non funzionale prioritario al pari di altri aspetti di qualità                                  

& Definizione di approcci e analisi dello status di avanzamento circa l'adozione di tecniche e metodologie specifiche per ogni fase di un canonico ciclo di vita  di un prodotto software

\\ \hline
\pagebreak
\multicolumn{3}{|l|}{\textbf{\textit{** Paper}}: "Fairness Analysis" in Requirement Assignments~\cite{finkelstein2008fairness}} 
\\ \hline
Finkelstein at al.            

& Identificare un processo standard di identificazione di specifica dei requisiti fair-oriented, in relazione con definizioni di fairness spesso contrastanti 

& Analisi dei requisiti correlati da fair trade-off modellata come problema di ricerca, con vincoli specifici dettati da differenti approcci matematici di Software Fairness 


\\ \hline

\rowcolor{Gray}
\multicolumn{3}{|l|}{\textbf{\textit{** Paper}}: Fairness Testing: Testing Software for Discrimination~\cite{galhotra2017fairness}} 
\\ \hline
 
\rowcolor{Gray}
 Yuriy Brun at al.             

& Definire nuovi approcci di testing fair oriented al fine di misurare se e in che modo i programmi effettuano discriminazioni                                

& Definizione dell'approccio Themis,  al fine di generare ed eseguire test suite per valutare, secondo alcune specifiche definizioni  di fairness, l'impatto di feature plausibilmente discriminanti in un tool 



\\ \hline
\multicolumn{3}{|c|}{\footnotesize \textbf{* Intelligenza Artificiale **Ingegneria Del Software}}
\\\hline
\caption{Software Fairness Related Work} % needs to go inside longtable environment
\label{tab:myfirstlongtable}
\end{longtable}
%\end{table} 



Si osserva come nell'ambito dell'intelligenza artificiale e del machine learning i fair-bias possono influenzare la creazione di dataset di addestramento, l'addestramento dei moduli di apprendimento (che di conseguenza soffriranno a loro volta di bias), e i risultati stessi di un processo di addestramento possono essere potenzialmente discriminanti per i gruppi minoritari \cite{vasudevan2020lift}. Per tutti questi motivi, la comunità di ricerca, nell'ambito dell'intelligenza artificiale è sempre più propensa a studiare e eliminare questi bias di tipo algoritmico, al fine di produrre soluzioni AI-Intensive, e Sistemi di Machine Learning che risentano sempre di meno delle problematiche connesse alla Fairness.\\

Recentemente la fairness nei software di machine learning ha destato notevole interesse anche nella comunità dell'ingegneria del software \cite{Ignorance&Prejudice}, per esempio grandi studiosi dell'ambito, quali Yuriy Brun and Alexandra Meliou dell'università del Massachusetts, affermano che numerose iniziative in diverse aree dell'ingegneria del software (quali: specifica dei requisiti, design, testing e verifica) necessitano di essere prese al fine di risolvere il problema. Altri studiosi invece descrivono la fairness come una vera e propria proprietà non funzionale per i sistemi di machine learning e che sostanziale effort di testing sia necessario al fine di scovare le vulnerabilità e violazioni di fairness all'interno dei moduli di machine learning.
\begin{center}
    \emph{Una panoramica completa di tutti i related work di Software Fairness \\ presentati nel documento è riportata in tabella 3.1}
\end{center}


\subsection{Fairness come oggetto di studio nell'ambito AI}
\subsubsection{Bias di dati come causa di discriminazioni di un modulo AI}

Rendere un modulo di machine learning, fair, significa renderlo innanzitutto indipendente dalle dipendenze correlate ai bias immessi, quindi una delle primissime attività su cui la ricerca si basa, è proprio l'individuazione di bias all'interno dei set di dati,i quali devono essere mitigati tramite intensive attività di pre-processing.\\  

Come affermato da Chakraborty et al.\cite{biasInML}  la principali cause dei bias, sono le decisioni prese a priori circa i dati che vengono selezionati per un modulo di Machine Learining e l'assegnazione di "label" che possono portare alla creazione di disparità di gruppo tra gli individui del dataset. Esempi di "etichette" possono derivare da attributi quali sesso, razza, età, status sociale, etc (volendo porre un esempio, gli individui del dataset possono essere "etichettati" in maniera discriminante come "ricco" o "povero" in base al guadagno netto mensile). Il loro algoritmo Fair-SMOTE che dati in input dataset e attributo sensibile, partiziona l'intero dataset in 4 sottogruppi (individui favoriti e privilegiati, favoriti e non privilegiati, individui non favoriti e privilegiati ed infine individui non favoriti e non privilegiati). Sinteticamente, da questa divisione iniziale, l'algoritmo va a generare nuovi \emph{data points}, unità discrete di informazione, quindi nuove entry del dataset, per ciascuno dei sottogruppi, ad eccezione di quello che risulta averne il numero maggiore. Come risultato, tutti e 4 i sottogruppi del dataset diventeranno della stessa taglia rispetto l'attributo sensibile (la stessa del gruppo più numeroso). Lo scopo di questo algoritmo, come altri due presenti nello stato dell'arte (Fairway di Chakraborty e Oprimized pre-processing for discrimination prevention di I.Guyon) è quello di fornire bias mitigation e quindi bilanciare il dataset di partenza (con attività di pre-processing), con il compromesso di condurre a priori un analisi sui dati di partenza che non sia complessa in termini di performances (misurata in termini di Recall e F-Measure). Statisticamente il loro algoritmo Fair-SMOTE è tra le soluzioni più promettenti effettuando data-balancing in media 200 volte più velocemente di altre soluzioni ed a seguito di studi empirici su più dataset è tra i più consigliati al fine di mitigare i bias all'interno di un dataset di addestramento.

\subsubsection{Fairness come conseguenza delle Feature e del Training Set}
Altri studi come quelli condotti da Zhang e Harmon, \cite{Ignorance&Prejudice}, affermano che la Fairness è naturalmente un problema specifico del dominio di utilizzo, ma che è comunque possibile generalizzare il concetto analizzando il numero di feature e l'ammontare dei dati di training. In particolare nel suddetto paper di ricerca, vengono riportati i risultati di uno studio empirico sull'impatto delle fattezze del  feature set e del dataset di training quando si cerca di sviluppare \textit{fair machine learning software}, ed in particolare viene valutate le implicazioni che questi aspetti hanno nel costruire \textit{Fair ML Models}. Per lo studio vengono utilizzati diversi datasets presenti e noti in letteratura (quali il COMPAS score, per la valutazione di recidività nel crimine) e differenti definizioni e metriche di fairness (riconducibili a quanto illustrato nel capitolo precedente). Applicando differenti considerazioni matematiche, si arriva ad affermare che avere modello ML con un grande numero di feature, aiuta a migliorare (secondo varie definizioni matematiche) i livelli di fairness d del 38\% rispetto la media e che un grande numero di dati possa provocare l'effetto contrario, ovvero una diminuzione sostanziale dei livelli di fairness.

\subsubsection{Diversità nella selezione dei dati con vincoli di Fairness}
Considerando che i dati sono generati e collezionati da tutti gli aspetti dell'attività umana, in domini come commercio, medicina e trasporti così come la misurazione scientifica, le simulazioni e il monitoring ambientale, è facile incorrere nella pratica di raggruppare i dati, in modo tale da garantire il principio di diversità, il quale resta uno degli elementi molti campi applicativi dell'Intelligenza artificiale come la Summarization, la Facility Location e i sistemi di raccomandazione. Ad oggi però non sono molti gli studi che mettono a confronto la Diversificazione con il concetto di Fairness, i quali sono strettamente correlati, ma modellano concetti differenti, la prima cerca di massimizzare la dissimilarità di items in un insieme di dati, mentre la seconda cerca di raggiungere specifici livelli di rappresentazione considerando diverse categorie e gruppi. \\ Moumoulidou at al. \cite{FairnessDefinitionExplained} osservano proprio come sia importante e non banale selezionare sotto-insiemi di di addestramento più differenti possibili (massimizzando la dissimilarità degli individui in ogni insieme), soprattutto qualora sia necessario raggiungere specifici livelli di rappresentazione di differenti categorie e gruppi, in altre parole il problema che il paper analizza è proprio quello di creare diversificazione tra gli individui di un dataset secondo specifici vincoli di Fairness (tipicamente definiti su attributi sensibili). Viene menzionato il più studiato, analizzato e frequentemente usato modello di diversificazione usato dalla comunità del Data Management, ovvero il Max-Min diversification model. Dopo aver introdotto il modello base, lo studio si concentra su una specializzazione del problema con l'introduzione di un numero k di vincoli di fairness prespecificati (per specificare i vincoli è possibile utilizzare per esempio la definizione di Statistical Parity). Quello che, infine, si osserva è che la \emph{fair - Max-Min Diversification} sia un esempio di algoritmo NP-Completo (conclusione ricavata dimostrando prima l'NP-Completezza del problema generale), in ragion di ciò vengono mostrate delle forti approssimazioni dell'algoritmo base che garantiscono la diversità in caso di gruppi non sovrapposti (ad intersezione vuota) \cite{moumoulidou2020diverse}.

\subsection{Fairness come oggetto di studio nell'ambito SE}
\subsubsection{Software Fairness come requisito non funzionale prioritario}
Come già osservato più volte all'interno del capitolo Stato dell'Arte, progettare e produrre fair software, sta diventando un ambito che interessa sempre di più il dominio dell'ingegneria del software, Yuriy Brun e Alexandra Meliou, \cite{brun2018software} osservano come la Software Fairness, debba essere un entità di "prima classe" in un tipico processo di ingegnerizzazione del software, al pari di altri aspetti non funzionali come qualità e sicurezza. In particolare, al fine di rendere immune un tool immune da discriminazioni, quindi ridurre quelli che sono i difetti intrinseci che diminuiscono i livelli di fairness, è senz'altro importante adottare buone pratiche di design e e algoritmi mirati, ma è necessario porre sullo stesso piano anche la necessità di supportare attività di "fairness testing". Al fine di misurare le discriminazioni software, è necessario identificare e riportare quelli che vengono definiti come "discrimination bugs", in modo tale da cambiare il codice o i dati che introducono tali discriminazioni. Cercando appunto di rispondere a tali problematiche, l'articolo evidenzia tutta una serie di challenge aperte in ciascuno degli aspetti chiave del ciclo di sviluppo del software \cite{brun2018software}:

\begin{enumerate}
    \item \emph{Requirement and Specification}: ponendo l'accento sulla moltitudine di definizioni emerse per il concetto di fairness algoritmica, e sulla correlata difficoltà di definire un software "fair" in maniera univoca, il paper osserva come la consistenza dei requisiti e le analisi correlate siano una una delle challenge aperte nell'ambito della Requirement Engineering quando si parla di Software Fairness, infatti, quando si considerano combinazioni di "fairness requirements" si può aver a che fare con più definizioni del concetto che possono essere contro intuitive e mutualmente esclusive, e allo stesso tempo analisi automatizzate possono identificare requisiti insoddisfabili o inconsistenti. Il risultato ultimo, infatti, è senz'altro la produzione di software soggetto a risultati inattesi e comportamenti inaspettati, per esempio si osserva, come un tool addestrato con la tecnica degli alberi di decisione derivato da un dataset con feature sensibili, fosse finito a discriminare in maniera molto forte sulla razza dei singoli individui. Al fine di evitare tali problematiche si sottolinea come l'analisi possa aiutare a comprendere come i requisiti di fairness influenzino gli altri requisiti (fairness trade-off) al fine ultimo di realizzare una specifica dei requisiti corretta;
    \item \emph{Architecture and design}: è noto come le inconsistenze tra le proprietà di design desiderate per i sistemi software siano comuni. Infatti, in generale una challenge di design aperta è proprio quella di creare tools che aiutino a modellare le architetture dei sistemi, identificando i conflitti. In particolare, nell'ambito del la software fairness, si osserva come una delle ricerche aperte, sia proprio quella di sviluppare stili di sviluppo e pattern di design, con l'obiettivo di trattare fairness trade-off ed identificazione fair design goals  in maniera semi-automatica, come già viene fatto per altre specifiche non funzionali, ad esempio tramite l'ottimizzazione multi-obiettivo. Il paper inoltre osserva come  per i sistemi ML-Intensive, il design di algoritmi fairness-aware possa produrre dei fair models soprattutto laddove lavorare con dati di training affetti da bias è critico. La ricerca da questo punto di vista è molto attiva, e molti algoritmi sono in sviluppo e molti framework di progettazione sono in sviluppo;
    
    \item \emph{Testing and Debugging}: È ormai noto come il primo metodo per assicurare la qualità del software sia il testing. Questa è la principale ragione per credere che ciò sia vero anche per la software fairness. In particolare il paper afferma come i Fairness bug siano comuni per sistemi con complessi input e output (si pensi alle forti dipendenze dalla lingua dei sistemi di Speach to Text, all'accuracy dei sistemi di riconoscimento facciale, strettamente dipendenti dalle informazioni demografiche come sesso e razza) e come questi sistemi siano la challenge più grande per la generazione di casi di test per questa tipologia di tool. Il paper evidenzia come il fairness testing richieda che i moduli vengano posti sotto test, svariate volte, e ricordando che il testing esaustivo è inapplicabile per sistemi complessi, sottolinea l'importanza di eseguire test con input similari. Si evidenzia infatti come l'ottimizzazione di esecuzione incrementale, sulla base dei test già eseguiti con input similari, possa, potenzialmente, ridurre i tempi di esecuzione dei test e aumentare l'applicabilità del fairness testing per i grandi sistemi. Similarmente, la prioritizzazione e selezione dei casi di test possono migliorare l'efficienza dei sistemi di fairness testing. Contestualmente si specifica anche la necessità per gli sviluppatori di identificare e rimuovere le "root causes" dei bias, ciò comporta come la ricerca si stia attivando al fine di fornire strumenti di debugging appositi da mettere a disposizione degli sviluppatori;
    
    \item\emph{Verification}: al pari della correttezza del software, il paper evidenzia come anche la verificabilità sia un goal altamente desiderabile per la software fairness. L'esecuzione multipla dello stesso codice che può portare ad output diversi (\textit{non determinismo}), la stretta dipendenza del concetto di fairness con l'esecutore (\textit{la multi utenza}) e la \emph{natura probabilistica} delle proprietà di fairness, sono tutti aspetti che riducono lo spettro di tecniche di verifica e validazione esistenti ed applicabili direttamente al problema. Quando si parla di fairness, è essenziale verificare il comportamento dei tool già durante lo sviluppo, perciò, si evidenzia come creare ambienti di runtime e tool di debugging mirati all'identificazione dei bug o warning di fairness sia essenziale, al fine di verificare formalmente il comportamento dei tool. Il problema principale però è nuovamente, identificare modi per codificare le definizioni di fairness come proprietà verificabili di un programma, ciò è strettamente connesso alla natura intrinseca delle metriche, alcune sono di tipo probabilistico e plausibilmente verificabili, altre però sono di natura diversa (e.g. casual reasoning e metriche strutturali). Tutto ciò rende la verifica una sfida di ricerca ancora molto aperta e avvincente.
\end{enumerate}

\subsubsection{Il "problema fairness" nella specifica dei requisiti}
Finkelstein at Al. \cite{finkelstein2008fairness}, nel 2008 hanno introdotto il concetto di fairness nell'ambito dell'analisi e ottimizzazione dei requisiti. Il lavoro è particolarmente interessante perché introduce modelli valutativi, basati su funzione di valutazione multi obiettivo, al fine di bilanciare i trade-off derivanti da differenti clienti. I modelli proposti adottano scenari semplificati al fine di bilanciare il problema della fairness tra le sue differenti definizioni, infatti il primo step che il paper cita mostra come sia possibile utilizzare le tecniche di "search based optimization" al fine di giungere ad un compromesso tra le varie definizioni di fairness in specifici contesti. L'esperimento poi dimostra come le tecniche di ricerca possano essere anche applicate a dataset reali e illustra come tali tecniche possano essere anche utilizzate per identificare i unfair bias intrinseci in tali dataset. Anche se un po' datato, questo paper evidenzia un problema che è tutt'ora attuale: aspetti intrinseci dello sviluppo software e delle tematiche AI-Intensive al giorno d'oggi mettono in luce ancora innumerevoli sfide nel campo dell'ingegnerizzazione dei requisiti e dei dati. Lo sviluppo di soluzioni AI-Intensive è strettamente influenzato da trade-off qualitativi tra cui quelli legati al mondo della fairness, e soluzioni di intelligenza artificiale, come tecniche di ricerca (e.g. algoritmi genetici), oppure modelli di ottimizzazione multi obiettivo, potrebbero sicuramente dare un'ottima risposta a queste esigenze.

\subsubsection{Il Testing in ambito Software Fairness}
Yuriy Brun et al. oltre ad essere famosi per aver posto l'accento all'emergente problema della Software Fairness, hanno condotto anche attività di ricerca specifiche come studi specifici circa il fairness testing. Uno dei loro studi più famosi del 2017 \cite{galhotra2017fairness}, propone un nuovo approccio di fair-testing, chiamato dai ricercatori Themis, al fine di misurare se e in che modo i programmi effettuano discriminazioni, focalizzandosi sulle casualità dei comportamenti discriminatori. L'approccio Themis genera test suite al fine di computare score inerenti la casual discrimination per particolari caratteristiche, e.g. secondo specifiche definizioni di fairness, il tool è in grado di generare uno score che determina quanto un sistema software discrimina contro razza ed età. Individuato un problema di discriminazione, Themis genera una test suite al fine di computare tutti i sets di caratteristiche che potrebbero essere alla base del problema. Fornendo, infine, in input al sistema di testing, una test suite manuale o auto generata, esso è in grado di verificare, su specifici input rappresentativi della popolazione, se effettivamente sono presenti feature del dataset discriminanti. L'obiettivo principale di Themis è quello di rispondere al problema di esecuzione del fairness testing per sistemi reali, (citato anche nel paper generale \cite{brun2018software}), infatti, le tre tecniche di ottimizzazione che il sistema di testing adotta, riducono il numero di test cases necessario a computare informazioni circa i gruppi sensibili più significativi di un dataset, con l'obiettivo di individuare le cause di discriminazione che influenzano il comportamento del tool sotto test.

\subsection{Riflessioni sullo stato dell'arte e sull'evoluzione della Software Fairness}

Visione comune del mondo della ricerca, è che il concetto di Software Fairness è ancora in evoluzione, negli ultimi anni molti studi emergenti, hanno rivalutato l'importanza del concetto cercando in vari modi di sistematizzarlo e formalizzarlo il più possibile. Come osservato in questo capitolo, sforzo principale della ricerca, è stato infatti, analizzare i vari aspetti che il concetto di Fairness racchiude, molte sono le metriche emerse che cercano di descrivere in maniera qualitativa o quantitativa i livelli di Fairness di uno specifico tool, come un generico modello di Machine Learning o una soluzione AI-Intensive.\\

Dagli studi analizzati, si osserva come prerogativa principale dei ricercatori nel campo dell'intelligenza artificiale, siano orientati attivamente per rendere dataset e tecniche AI-based sempre più conformi alle definizioni semantiche dell'una o l'altra metrica. A problematiche specifiche dell'intelligenza artificiale, quali il rilevamento di fair-bias nei dataset oppure applicare operazioni di fair-diversification in fase di pre- processing, l'ingegneria del software sta cercando di sistematizzare il concetto in modo tale da rendere ai Data Scientist più agevole il trattamento della Fairness. Come osservato, negli ultimi anni sono nati molti quesiti che pongono il problema del software eticamente corretto sotto una nuova luce, ovvero quella di vero e proprio requisito non funzionale prioritario per una soluzione AI-Intensive, di conseguenza, la comunità ingegneristica, cerca di considerare, come per ogni specifica non funzionale, quali sono le tecniche e metodologie migliori per trattare la Fairness in ogni fase del ciclo di vita del software. Studi come quelli inerenti al Fairness Testing, oppure le tecniche Search-based per il bilanciamento dei requisiti di fairness tra definizioni strutturalmente e semanticamente contrastanti, sono solo alcuni degli esempi di soluzioni che gli studiosi di Ingegneria del Software hanno formulato negli ultimi anni, e molti altri saranno sicuramente proposti, considerando che la ricerca in ambito di Software Fairness, con tutta probabilità andrà avanti, in maniera molto pronunciata nei prossimi anni.\\\\

Ma guardare al futuro della ricerca significa quindi capire in che direzione sarà opportuno far evolvere il concetto di Software Fairness, al fine di renderlo sempre più vicino al mondo delle aziende di sviluppo, ed in particolare, nel contesto AI, sempre più vicino al mondo dei Data Scientist e dei modelli AI-Intensive per problematiche reali. Ovviamente è molto complesso sistematizzare il concetto di Software Fairness nella pratica lavorativa, così come è stato fatto fin ora in modo teorico, molte sono le variabili in gioco:
\begin{itemize}
    \item Quanto e come figure come Data Scientist, Ingegneri del Software in ambito AI e figure manageriali si approccino al problema? 
    \item Quali sono le definizioni più adottate in ambito lavorativo per i problemi fair-critical negli ambiti in cui lo sviluppo AI-Intensive si concentra oggi giorno? Quali lo saranno in futuro? 
    \item Esistono già best-practices da seguire per garantire alti livelli di fairness di un modello di machine learning? se si, secondo quali definizioni? 
\end{itemize}
 Di quesiti di questo tipo se ne possono fare tanti, e ovviamente rispondere a tutto in maniera definitiva è un qualcosa che va al di fuori degli obiettivi di questo lavoro di tesi. Però in un contesto così ancora inesplorato, il budget a disposizione, permette sicuramente di partire con l'interpellare i diretti interessati. Tramite tecniche empiriche mirate si tenterà, infatti, di capire se diretti interessati (e.g. Data Scientist e Project Manager) si approcciano attualmente al problema, se effettivamente le definizioni teoriche di fairness hanno effettivo riscontro sul campo e se e come processi ingegneristici fair-orinted vengono applicati durante il ciclo di sviluppo di una soluzione AI-Intensive con obiettivo ultimo di creare una vera e propria raccolta riassuntiva dello status della pratica che abbia un duplice obiettivo a lungo termine, ovvero:
 
 \begin{itemize}
     \item Provare a suggerire  ai ricercatori quali sono i punti di forza e di debolezza per futuri lavori plausibilmente sempre più vicini al mondo del lavoro;
     \item Cercare di avvicinare sempre di più gli esperti del settore AI al contesto della fairness, in modo tale che adottino sempre di più processi di sviluppo fair-oriented.
 \end{itemize}


\newpage