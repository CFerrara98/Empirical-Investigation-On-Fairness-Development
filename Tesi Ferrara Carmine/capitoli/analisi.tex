\chapter{Analisi dei dati e  risultati di ricerca} %\label{1cap:spinta_laterale}
% [titolo ridotto se non ci dovesse stare] {titolo completo}
%

	\definecolor{perla}{rgb}{1.0, 0.97, 0.91}
     \section{Data Cleaning}
    
    Come osservato precedentemente, il survey Prolific, ha raccolto un totale di 203 risposte totali. Prima di partire con l'effettiva analisi dei dati però, si è reso necessario validare la consistenza e l'integrità delle risposte ricevute. Inizialmente 19 sottomissioni sono state considerate inattendibili e quindi rimosse dal dateset di partenza per i seguenti motivi:
    
    \begin{itemize}
        \item 17 sono state considerate inattendibili a causa di risposte errate alle domande poste come attenction check di verifica, quindi successivamente eliminate dal dataset originale.
        \item 2 risposte sono state eliminate, osservando incongruenze con l'identificativo Prolific immesso.
    \end{itemize}
     
    Delle 184 sottomissioni restanti, c'è inoltre da considerare che 68 partecipanti hanno dichiarato di non aver alcuna esperienza con lo sviluppo di moduli AI-intensive, quindi sono stati direttamente condotti alla sezione di chiusura del Survey elettronico, anche se mantenute nel dataset di partenza, tali risposte (per strutturazione intrinseca del survey) non possedevano informazioni utili per rispondere i quesiti di ricerca, ma data la presenza di quesiti aperti e contatti utili, esse sono state trasferite in un dataset secondario da utilizzare per successive fasi di approfondimento. I restanti 116 partecipanti all'indagine hanno invece dichiarato di avere effettivamente esperienza con lo sviluppo di soluzioni AI-Intensive, quindi le risposte relative sono state ritenute utili al fine di elaborare i risultati dello studio in risposta agli obiettivi di ricerca prefissati precedentemente.\\  
    
    
    Da notare inoltre come 18 partecipanti dei 116 restanti (con esperienza nello sviluppo di soluzioni AI-Intensive), abbiano lasciato a disposizione un loro contatto email per eventuali interviste future, o per ottenere nuovi aggiornamenti circa l'elaborazione dei risultati.  
    
    \section{Pre-processing}
    Prima di procedere con il vero e proprio lavoro di analisi, è stato necessario applicare alcune piccole trasformazioni ai dati grezzi, e consequenzialmente prima di illustrare il lavoro di analisi, si riportano riferimenti alle trasformazioni applicate al dataset, utili a interpretare i risultati ottenuti in maniera più agevole.
    
    \subsection{Mapping tra quesiti del survey e obiettivi di ricerca}
    
    Al fine di comprendere meglio quali quesiti, posti ai partecipanti all'indagine empirica, forniscano dati utili a rispondere ad uno specifico quesito di ricerca, il dataset di risposte iniziale, è stato diviso in 5 dataset più piccoli, ciascuno corrispondente ad uno specifico sub-goal di ricerca formalizzato precedentemente. La successiva tabella 5.1 riassume i dettagli dell'attività di mapping.
    
    \begin{longtable}{| p{.90\textwidth} |} 
        
        \hline
        \rowcolor{Gray}
        \textbf{\textit{{RQ1}}}:  Quali sono i migliori approcci e definizioni per trattare la fairness in un contesto lavorativo?\\
        
        \hline 
         Secondo te, quali dei seguenti aspetti rappresentano la definizione generica di fairness fornita in precedenza?
         \\ \hline
        Considerando la tua esperienza lavorativa, quanto i seguenti (approcci) sono trattati?
        \\ \hline
        Generalmente utilizzi altri approcci per lavorare con il concetto di Software Fairness?
        \\ \hline
        
        \rowcolor{Gray}
        \textbf{\textit{{RQ2}}}:  Com'è generalmente composto un team lavorativo per lo sviluppo di moduli ML-Intensive Fair Critical?\\
        
        \hline 
         Considerando i seguenti ruoli (professionali), chi ha impatto sulle scelte inerenti la software fairness?
       
        \\ \hline
        \rowcolor{Gray}
        \textbf{\textit{{RQ3}}}:  Quanto il concetto di software fairness è importante se paragonato ad altri aspetti non funzionali?\\
        
        \hline 
         Considerando i seguenti aspetti (funzionali e non funzionali) dello sviluppo software, quanto li ritieni importanti se comparati alla fairness?
         
        \\ \hline
        \rowcolor{Gray}
        \textbf{\textit{{RQ4}}}:  In quali fasi di una tipica pipeline di Machine Learning è importante adottare strategie per garantire alti livelli di fairness?\\
        
        \hline 
         Considerando una generica pipeline di machine learning (come la seguente - figura 4.1), quanto consideri l’equità come un aspetto rilevante per ciascuna delle seguenti fasi nel tuo contesto lavorativo?
         
         \\ \hline
        Quali tool utilizzi (se previsti) per trattare la fairness in una pipeline di machine learning ?
        
        \\ \hline
        \rowcolor{Gray}
        \textbf{\textit{{RQ5}}}:  Quanto le compagnie di sviluppo ML-Intensive, sono mature nel trattare il concetto di fairness come un requisito non funzionale?\\
        
        \hline 
        In quale dei seguenti livelli di maturità, classificheresti il tuo ambiente lavorativo circa il trattamento della fairness?
        \\ \hline
        \caption{Survey Question \& Research goal mapping} % needs to go inside longtable environment
    \label{tab:myfirstlongtable}
    \end{longtable}
    
    \subsection{Abbreviazioni e trasformazioni di scala}
    
    Considerando che molti concetti formalizzati sul questionario, sono stati espressi in forma discorsiva per garantirne una maggiore comprensione ai partecipanti all'indagine, si è reso necessario convertirli prima della fase di aggregazione dei dati, in valori compatibili agli strumenti automatici di analisi utilizzati. I quesiti specifici, per cui si è resa necessaria questa attività, sono state quindi formalizzate due tipologie di scale, una quantitativa o riassuntiva (composta da alias per il concetto espresso in forma discorsiva) e una qualitativa (contenente le vere e proprie opzioni di risposta alla domanda di riferimento).\\
    
    Le successive tabelle riassumono quindi per intero le trasformazioni di scala applicate.\\\\
    
    
    \subsubsection{Definizioni di fairness e abbreviazioni}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ1 - Quesito: Secondo te, quali dei seguenti aspetti rappresentano la definizione generica di fairness fornita in precedenza?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
     \begin{longtable}{| p{.30\textwidth} | p{.55\textwidth} |} 
      
      \hline\textbf{\textit{Valore Riassuntivo}} & \textbf{\textit{Valore qualitativo (risposte)}}
       
        
        \\ \hline
        \rowcolor{Gray}
        Definizioni Probabilistiche        
        
        &   Treating similar individuals in a way that they are equally likely to receive a specific outcome

        
        \\ \hline


        Definizioni basate su similarità matematica      
        
        &  Do not favor certain subjects over others on the basis of sensitive attributes, e.g., race, gender, etc.




        \\ \hline
        \rowcolor{Gray}
        Definizioni basate su casual reasoning       
        
        &  Taking decisions by protecting individuals and groups from mistreatments

        
        \\ \hline
        \caption{Mapping tra le tipologie di definizione di fairness e la loro forma discorsiva} % needs to go inside longtable environment
        \label{tab:myfirstlongtable}
    \end{longtable}
    
    
     \subsubsection{Approcci al trattamento della fairness e abbreviazioni}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ1 - Quesito: Considerando la tua esperienza lavorativa, quanto i seguenti (approcci) sono trattati?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
     \begin{longtable}{| p{.30\textwidth} | p{.55\textwidth} |} 
      
      \hline\textbf{\textit{Valore Riassuntivo}} & \textbf{\textit{Valore qualitativo (risposte)}}
       
        
        \\ \hline
        \rowcolor{Gray}
        Approccio 1       
        
        &   We focus on guaranteeing high probability to obtain ethically correct outcomes regardless of sensitive features


        
        \\ \hline


        Approccio 2      
        
        &  We focus on guaranteeing that machine learning predictions are not going to discriminate by sensitive features





        \\ \hline
        \rowcolor{Gray}
        Approccio 3      
        
        &  We model the relation between attributes and outcomes, verifying that the outcome does not depend on sensitive attributes


        
        \\ \hline
        \caption{Mapping tra le tipologie di approcci alla  fairness e la loro forma discorsiva} % needs to go inside longtable environment
        \label{tab:myfirstlongtable}
    \end{longtable}
    
    \subsubsection{Cambiamenti di scala per l'applicabilità di definizioni e approcci alla fairness}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ1 - Quesito: Secondo te, quali dei seguenti aspetti rappresentano la definizione generica di fairness fornita in precedenza?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ1 - Quesito: Considerando la tua esperienza lavorativa, quanto i seguenti (approcci) sono trattati?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
     \begin{longtable}{| p{.25\textwidth} | p{.35\textwidth} |} 
      
      \hline\textbf{\textit{Scala quantitativa}} & \textbf{\textit{Valore qualitativo (risposte)}}
       
        
        \\ \hline
        \rowcolor{Gray}
        1       
        
        &  Not at all


        
        \\ \hline


        2     
        
        & Slightly



        \\ \hline
        \rowcolor{Gray}
        3     
        
        &  Neutral

        
        \\ \hline


        4    
        
        & To a great extent



        \\ \hline
        
        \rowcolor{Gray}
        5
        
        &  Extremely

        \\ \hline
        \caption{Scale qualitativa e quantitativa per la valutazione di definizioni e approcci} % needs to go inside longtable environment
        \label{tab:myfirstlongtable}
    \end{longtable}
\newpage
\subsubsection{Cambiamenti di scala circa l'impatto professionale nel trattamento della fairness}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ2 - Quesito: Considerando i seguenti ruoli (professionali), chi ha impatto sulle scelte inerenti la software fairness?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
     \begin{longtable}{| p{.25\textwidth} | p{.35\textwidth} |} 
      
      \hline\textbf{\textit{Scala quantitativa}} & \textbf{\textit{Valore qualitativo (risposte)}}
       
        
        \\ \hline
        \rowcolor{Gray}
        1       
        
        &  Very low impact



        
        \\ \hline


        2     
        
        & Below average impact




        \\ \hline
        \rowcolor{Gray}
        3     
        
        &  Average impact


        
        \\ \hline


        4    
        
        & Above average impact




        \\ \hline
        
        \rowcolor{Gray}
        5
        
        &  Very high impact


        \\ \hline
        \caption{Scale qualitativa e quantitativa per la valutazione dell'impatto professionale} % needs to go inside longtable environment
        \label{tab:myfirstlongtable}
    \end{longtable}
    
\subsubsection{Cambiamenti di scala circa la valutazione di importanza della fairness rispetto altri NFR}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ3 - Quesito: Considerando i seguenti aspetti (funzionali e non funzionali) dello sviluppo software, quanto li ritieni importanti se comparati alla fairness?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
     \begin{longtable}{| p{.25\textwidth} | p{.40\textwidth} |} 
      
      \hline\textbf{\textit{Scala quantitativa}} & \textbf{\textit{Valore qualitativo (risposte)}}
       
        
        \\ \hline
        \rowcolor{Gray}
        -2    
        
        &  Less important than fairness


        \\ \hline


        -1     
        
        & A bit less important than fairness

        \\ \hline
        \rowcolor{Gray}
        0    
        
        &  Neutral

        \\ \hline


        1    
        
        & A bit more important than fairness





        \\ \hline
        
        \rowcolor{Gray}
        2
        
        &  More important than fairness



        \\ \hline
        \caption{Scale qualitativa e quantitativa per la valutazione dei fairness trade-offs} % needs to go inside longtable environment
        \label{tab:myfirstlongtable}
    \end{longtable}
    
    
    
    \subsubsection{Cambiamenti di scala circa l'utilità di applicazione di strategie Fair-Oriented in una pipeline ML}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ4 - Considerando una generica pipeline di machine learning (come la seguente - figura 4.1), quanto consideri l’equità come un aspetto rilevante per ciascuna delle seguenti fasi nel tuo contesto lavorativo?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
     \begin{longtable}{| p{.25\textwidth} | p{.40\textwidth} |} 
      
      \hline\textbf{\textit{Scala quantitativa}} & \textbf{\textit{Valore qualitativo (risposte)}}
       
        
        \\ \hline
        \rowcolor{Gray}
        1    
        
        &  Not at all

        \\ \hline


        2    
        
        & Slightly


        \\ \hline
        \rowcolor{Gray}
        3   
        
        &  Neutral

        \\ \hline


        4   
        
        & Very


        \\ \hline
        
        \rowcolor{Gray}
        5
        
        &  Extremelly


        \\ \hline
        \caption{Scale qualitativa e quantitativa per l'impatto di fairness su una Pipeline di Machine Learning} % needs to go inside longtable environment
        \label{tab:myfirstlongtable}
    \end{longtable}
    
    \newpage
    \subsubsection{Livelli di maturità aziendale e spiegazine relativa}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ5 - In quale dei seguenti livelli di maturità, classificheresti il tuo ambiente lavorativo circa il trattamento della fairness?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
     \begin{longtable}{| p{.20\textwidth} | p{.60\textwidth} |} 
      
      \hline\textbf{\textit{Livello}} & \textbf{\textit{Spiegazione}}
       
        
        \\ \hline
        \rowcolor{Gray}
        Livello 0    
        
        &  We do not treat software fairness

        \\ \hline


       Livello 1    
        
        & We occasionally treat software fairness,  but related processes are disorganized and even chaotic



        \\ \hline
        \rowcolor{Gray}
        Livello 2   
        
        &  We regularly treat fairness and related processes are established, defined and documented


        \\ \hline


        Livello 3   
        
        & We regularly treat fairness and it develops its own standard fairness management processes



        \\ \hline
        
        \rowcolor{Gray}
        Livello 4
        
        &  We regularly treat fairness and it monitors and controls its own fairness related processes through data collection and analysis


        \\ \hline


        Livello 5  
        
        & We regularly treat fairness and fairness related processes are constantly improved through monitoring feedback


        \\ \hline
        \caption{Scale qualitativa e quantitativa per l'impatto di fairness su una Pipeline di Machine Learning} % needs to go inside longtable environment
        \label{tab:myfirstlongtable}
    \end{longtable}
    
    \newpage
    \section{Analisi dei dati}
    Una volta realizzate le dovute trasformazioni di scala e la sistematizzazione dei concetti, il campione empirico, risulta quindi pronto per essere effettivamente analizzato. Ovviamente per la fase di analisi, è stato necessario selezionare un opportuno tool, che automatizzasse l'aggregazione statistica de dati e la formulazione di grafici descrittivi, tra le varie alternative disponibili, a tale scopo è stato selezionato il linguaggio statistico R, tramite il relativo tool di utilizzo RStudiio e la libreria ggplot2 per la formulazione di grafici descrittivi.
    
    \subsection{Composizione del campione}
    
    Prima di cominciare effettivamente con l'analizzare i risultati specifici di ogni goal di ricerca, è opportuno effettuare qualche considerazione sul campione di 116 individui considerati validi dopo la fase di data cleaning. \\

    
    \begin{figure}[h]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/Background/Survey_Partecipants_Provenience.png}
        \caption{Distribuzione continentale del campione di analisi}
    \end{figure}
    
    Come osservabile dalla figura 5.1, il campione di analisi, è principalmente concentrato tra Europa e Africa, ciò significa appunto che le deduzioni successive, questo dettaglio non è trascurabile, dato che la generalizzabilità dei risultati, potrebbe essere messa in discussione, per aree geografiche rappresentate in maniera minore. Ad ogni modo, questo prima considerazione, è senz'altro da attribuire al fatto che Prolific è essenzialmente una piattaforma di origine britannica, quindi maggiormente pubblicizzata in Europa. Nel dettaglio, 79 partecipanti hanno dichiarato di provenire dall'Europa, 27 dall'Africa, 6 dal Nord America, 2 dall'Asia, 1 dal Sud America, mentre 1 ha preferito non dichiarare la sua provenienza.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/Background/Survey_Partecipants_Gender.png}
        \caption{Distribuzione di gender nel campione di analisi}
    \end{figure}
    
    Analogamente alla provenienza, con la figura 5.2 è possibile effettuare qualche considerazione analoga, con l'identificazione di gender dei partecipanti. la presenza di tale quesito è stato ampiamente discussa in fase di progettazione del questionario, dato che essa è spesso un informazione da ritenere altamente discriminatoria se non non richiesta nel modo giusto, a tale scopo, è stata introdotta l'opzione di controllo Prefer Not To Say, che tuttavia è stata selezionata solo da un partecipante su 116. 80 invece si identificano nel gender maschile, 33 in in quello femminile, 1 in entrambi i precedenti, 1 come Transgender e donna contemporaneamente.\\
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/Background/Survey_Partecipants_Age.png}
        \caption{Distribuzione dell'età nel campione di analisi}
    \end{figure}
    
   Considerando invece la figura 5.3 è possibile notare, come la maggioranza del campione, si concentri maggiormente tra i 18 e i 30 anni, più nello specifico 79 partecipanti si colloca in questo range, 34 invece sono i partecipanti appartenenti alla fascia di età tra i 31 e i 50 anni, portando quindi anche maggiore maturità professionale al campione, mentre infine 3 partecipanti dichiarano di possedere più di 50 anni.
   
   \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/Background/Survey_Partecipants_Education.png}
        \caption{Distribuzione del livello di studi nel campione di analisi}
    \end{figure}
   Fin dall'inizio della fase di progettazione del Survey, al fine dell'attendibilità delle informazioni ricevute è stata considerato come fattore di rilevante importanza il livello di studio medio dei partecipanti, preferendo in particolare un alto livello di studio (laurea triennale e superiori), dalla figura 5.4 è osservabile come tale tendenza sia rispettata da quasi tutti i partecipanti all'indagine, nel dettaglio su 116 partecipanti totali, 65 dichiarano di possedere almeno un titolo di studi triennale, 35 invece dichiarano di aver conseguito un titolo magistrale, 4 risultano aver completato anche il dottorato di ricerca, mentre i restati 12 si dividono in 11 con un diploma di istruzione superiore e un singolo con diploma professionale.\\
   
   Continuando poi con la posizione e il ruolo professionale dei partecipanti, è possibile notare come effettivamente, il campione degli intervistati sia abbastanza variegato e corrispondente ai requisiti di appartenenza precedentemente fissati. 
   
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/Background/Survey_Partecipants_Occupation_Position.png}
        \caption{Livello di occupazione professionale dei partecipanti all'analisi}
    \end{figure}
    
    In particolare, come osserva la figura 5.5, la maggioranza dei partecipanti ricopre un ruolo attivo in una compagnia, nel dettaglio è possibile dividere questa fetta in 51 impiegati aziendali junior, 27 impiegati aziendali Senior, e 12 Project Manager e 2 come Top-Manager  in azienda. Tra i restanti, 11 si dichiarano come lavoratori autonomi. Tra i restanti, ci sono figure professionali singole, che dichiarano di ricoprire più di una posizione professionale contemporaneamente o altri ruoli quali sviluppatore open source, infine risultano esserci 5 partecipanti che dichiarano di avere altra posizione altro rispetto a quelle specificate, mentre solo 2 risultano disoccupati al momento della sottomissione.  
    
      \begin{figure}[h!]
        \centering
        \includegraphics[width=0.8\textwidth]{figure/Analisi/Background/Survey_Partecipants_Roles.png}
        
        \caption{Ruoli professionali dei partecipanti all'analisi}
    \end{figure}
    
    Dando invece uno sguardo ai ruoli professionali, è possibile osservare come circa la metà o poco più dei partecipanti, abbia affinità con l'ingegneria del software, l'ingegneria dei dati, e il management, leggermente meno ampia è la presenza di ruoli affini a posizioni quali Data Science o altri ruoli professionali, mentre in media. Mentre è di rilievo considerare che tutti i partecipanti hanno in media dai 2 ai 10 anni di esperienza nel ruolo indicato. Da un punto di vista empirico, l'eterogeneità del campione è senz'altro molto positiva dato che sia per via dei differenti ruoli, che per i variegati livelli di occupazione, è possibile dare più valenza statistica alle successive informazioni estratte dai dati grezzi.
    
    \subsection{Fairness in pratica, come rispondere al quesito generale?}
    
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					
                    	\textit{RQ -  In che modo il concetto di Software Fairness è attualmente percepito nell'ambiente lavorativo ML-Intensive?}
				
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
	Come più volte osservato più volte, cercar di raccogliere dati utili a rispondere in maniera formale a questo unico macro-quesito, può essere qualcosa di generalmente complesso e forse anche riduttivo da proporre per un concetto così variegato come quello di Software Fairness, sicuramente considerando l'andamento generale dell'analisi dei dati, così come viene riportato nei successivi paragrafi, è senz'altro facile capire che, come stabilito dalla ricerca Fairness, è un concetto estremamente variabile ed in evoluzione, quindi così come per la fase di progettazione, anche quella di analisi risulta essere più assimilabile se scomposta in sotto-punti speculari ai sub-goal di ricerca definiti. 
    
    \subsection{Applicabilità di definizioni e approcci per fairness}
    
    \begin{center}
			\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					\textit{RQ1 - Quali sono i migliori approcci e definizioni per trattare la fairness in un contesto lavorativo?}
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
    
     \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ1/Hist_Fairness_Deginitions_Applicability.png}
        \caption{Definizioni di fairness in ambito lavorativo}
    \end{figure}
    
    Data l'ampia variabilità del concetto, non è corretto porre un quesito di ricerca che voglia definire univocamente il concetto o la definizione \emph{aziendale} di software fairness. Da un punto di vista analitico, la figura 5.7, dimostra infatti come per gli esperti, tutte le tipologie di definizioni e metriche teoriche, possono essere ampiamente applicate in casi reali di sviluppo, per tutte e tre i gruppi di definizione, i livelli 4 e 5 delle scale di applicabilità superano il 50\% dei consensi, a seconda dei contesti di applicazioni e dei requisiti specifici del modulo ML. Da un'analisi più attenta, però si può osservare come i partecipanti all'indagine ritengano lievemente più applicabili i gruppi di definizione e metriche basati su \textbf{similarità matematica}, rispetto a quelli definiti in \textbf{puri termini probabilistici} piuttosto che quelli basati su \textbf{relazioni causali tra features e outcome}.
    
      \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ1/Hist_Fairness_Approaches_Applicability.png}
        \caption{Approcci al concetto di fairness in ambito lavorativo}
    \end{figure}
    
    Contrariamente se si osservano gli approcci pratici proposti, si osserva facilmente come in azienda prevalga facilmente l'idea che \textbf{i livelli alti di Software Fairness}, vadano ricercati cercando di \textbf{ridurre le dipendenze tra gli attributi sensibili e i risultati predetti in fasi di data preparation} (approccio 1), e quindi specularmente nel \textbf{cercare di constatare che i risultati stessi non dipendano da feature sensibili} in fase di validazione (approccio 3). Meno percepito come applicabile, risulta essere invece il secondo approccio proposto, ovvero concentrarsi nel garantire che il learner non effettui valutazioni discriminatorie sulla base di feature sensibili. Ciò è probabilmente da attribuire, alla complessità di questa seconda opzione proposta, non facilmente intuibile e difficilmente analizzabile rispetto le altre due.\\
    
    Ovviamente anche in azienda, il concetto di fairness non si limita a quanto espresso in teoria, esso può essere visto a vari livelli di dettaglio, e soprattutto in riferimento al dominio di utilizzo. A dimostrazione di ciò, risultano interessanti molte risposte ottenute al terzo quesito (risposta aperta breve) progettato per questo primo sub-goal di ricerca, ovvero: Generalmente utilizzi altri approcci per lavorare con il concetto di Software Fairness?
	
	Segue un breve report di alcune delle considerazioni più interessanti:
	
	\begin{itemize}
	    \item Approcci Domain Specific - Provare a rendere i risultati disponibili a persone con disabilità, quindi in sostanza, renderli equi per tutti; 
	    \item Metodologie Empiriche - Condurre survey e ottenere in cosiderazioni opinioni diverse;
	    \item Ottimizzazione nella gestione dei dati - Ottenere dei dati di training sensibili abbastanza affinchè i risultati siano il meno possibile discriminanti;
	    \item Analisi di correlazione - Analizzare la correlazione forte-debole tra risultati e features;
	\end{itemize}
    
	
	
	\begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=Gray]
    			\begin{minipage}{\textwidth}
    				\textit{\faKey  \textbf{ Risultato di ricerca 1}}\\
    				In ambienti di sviluppo ML-Intensive, definizioni e metriche formali basate su similarità matematica o approcci pratici basati sull'analisi di dipendenze tra feature sensibili risultano essere lievemente più applicabili rispetto altri noti in letteratura, in aggiunta i professionisti suggeriscono altre tecniche specifiche per il trattamento di fairness: quali metodologie empiriche o correlazione statistica.
    			
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
	
	
	
    \subsection{Impatto professionale per il trattamento della fairness}
	\begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					\textit{RQ2 - Come è composto generalmente un team lavorativo per lo sviluppo di moduli ML-Intensive Fair Critical?}
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	
	  \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ2/RoleImpactBoxPlot1.png}
        \caption{Ruoli professionali durante lo sviluppo Fair Oriented 1/2}
    \end{figure}
    
     \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ2/RoleImpactBoxPlot2.png}
        \caption{Ruoli professionali durante lo sviluppo Fair Oriented 2/2}
    \end{figure}
	 
	 
	 Osservando le figure 5.9 e 5.10, è facilmente intuibile, come \textbf{tutti i ruoli professionali} proposti, vengano considerati cruciali durante lo sviluppo di una soluzione fair critical, infatti il valore mediano per ogni figura professionale si attesta almeno a 4 (secondo valore di rilevanza nella scala precedentemente illustrata). Volendo entrare più nel dettaglio, è possibile osservare come \textbf{manager} ed \textbf{esperti specifici}, siano figure estremamente di rilievo e utili nella gestione degli aspetti di equità di un modulo di Machine Learning. Risultano molto simili sono invece attribuibili a figure come Data Scientist, Data Engineer e Ingegneri del software, a dimostrazione del fatto che, anche in un contesto reale, mantenere alta la sinergia tra queste due branche dello sviluppo ML-Intensive sia cruciale soprattutto in contesti molto specifici come quello della fairness. Da notare come ci sia una leggera presenza di outliers per ruoli come Analisti ed Esperti di fairness (basso impatto in controtendenza con il resto del campione), ma ciò è da considerare come fattore eccezionale, dato che tali figure possono essere comunque molto specifiche e magari non presenti in ogni ambiente lavorativo dei partecipanti all'indagine.
	 
	 
	 \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=Gray]
    			\begin{minipage}{\textwidth}
    				\textit{\faKey  \textbf{ Risultato di ricerca 2}}\\
    			 Per la gestione di problematiche etiche durante lo sviluppo di moduli di machine learning, risultano essere essenziali figure di management che monitorino i livelli di fairness del sistema, oltre che il coinvolgimento diretto di esperti specifici durante lo sviluppo fair oriented. Si nota anche come figure professionali trasversali quali Data Scientists e Ingegneri del Softaware abbiano responsabilità equiparabili nel trattamento delle specifiche etiche.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
	
	
    \subsection{Confronto tra fairness con altre caratteristiche qualitative non funzionali}
    
    \begin{center}
		\hspace*{-5mm}\begin{tikzpicture}
			\node [mybox] (box){%
				\begin{minipage}{.70\textwidth}
					\centering
					\textit{RQ3 - Quanto il concetto di software fairness è importante se paragonato ad altri aspetti non funzionali?}
				\end{minipage}
			};
		\end{tikzpicture}%
	\end{center}
	Come visto più volte, analizzare la fairness, come un vero e proprio attributo non funzionale di prima classe sia la chiave di lettura al fine di eliminare fault etici latenti in un modulo di machine learning \cite{brun2018software}. Ovviamente chiedere richiedere tale sforzo ai professionisti è un qualcosa di molto ostico soprattutto mettendo a paragone Fairness, con requisiti molto più standardizzati, quali accuracy o sicurezza. Ma analizzando i risultati, è possibile provare ad osservare qualche deduzione interessante.\\
	
	N.B. nei successivi diagrammi, sono state rappresentate le densità delle singole risposte dei partecipanti, in blu si può notare la fetta di persone che considera uno specifico requisito altamente meno importante rispetto a Fairness (-2 della scala quantitativa) , in verde è racchiusa la parte di campione che considera fairness leggermente più importate rispetto all'altro requisito analizzato dal diagramma (-1 della scala quantitativa), in giallo quella che considera fairness rilevante quanto il requisito confrontato (0 nella scala quantitativa), in arancione quella fetta di campione che considera il requisito in analisi lievemente più importante rispetto a fairness (+1), mentre in rosso gli individui che considerano il requisito specifico altamente più importante di fairness (+2).
	\begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/1.Usability_vs_Fairness.png}
        \caption{Fairness vs Usabilità}
    \end{figure}
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/2.Reliability_vs_Fairness.png}
        \caption{Fairness vs Affidabilità}
    \end{figure}
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/3.Performance vs. Fairness.png}
        \caption{Fairness vs Performances}
    \end{figure}
    
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/4.Supportablity vs. Fairness.png}
        \caption{Fairness vs Supportabilità}
    \end{figure}
    
    
    Come osservabile dalle figure 5.11 alla 5.14, aspetti non funzionali standard, del noto modello furps+ (usabilità, affidabilità, performance e supportabilità), sono aspetti che per la maggior parte del campione, risultano essere di misura più rilevanti di fairness, in particolare si osserva che per aspetti particolarmente critici per i moduli di intelligenza artificiale, quali \textbf{affidabilità e performances}, la fetta di persone che li considera \textbf{estremamente più importanti}, sale anche \textbf{sopra le 25/30 unità}, per diminuire leggermente negli altri aspetti riportati. È però da tener conto come quasi tutti gli aspetti del modello iniziale analizzato (Furps+), come fairness, siano altamente variabili a seconda delle necessità del tool specifico, infatti, osservando come la maggior parte dei partecipanti consideri \textbf{usabilità e supportabilità} leggermente più impattanti nello sviluppo ml-intensive, rispetto fairness, si può sicuramente lasciare margine di confronto tra questi attributi e fairness a seconda delle specifiche esigenze.
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/5.Accuracy vs. Fairness.png}
        \caption{Fairness vs Accuracy}
    \end{figure}
    
     \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/6.Security and Safety.png}
        \caption{Fairness vs Sicurezza}
    \end{figure}
    
    Il discorso cambia di misura per aspetti già più specifici per un canonico modello di machine learning, infatti sia per accuracy che sicurezza, nessuno dei partecipanti ha ritenuto opportuno indicarli come ampiamente meno importanti rispetto alla fairness, inoltre per questi due aspetti anche la finestra di voto che va da -1 a 0, risulta essere molto ristretta. Sia \textbf{l'accuracy che la Sicurezza, sono da considerare da lievemente ad estremamente più rilevanti} rispetto a fairness sulla base del campione. Ciò molto probabilmente è dovuto al fatto che chi lavora quotidianamente con questi sistemi, in maniera quasi automatica si troverà a lavorare a problematiche di accuracy o sicurezza del modello, cosa non sempre così semplice (almeno allo stato attuale delle cose) per fairness. 
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/7.Maintenability and Retraining.png}
        \caption{Fairness vs Manutenibilità e Retraining}
    \end{figure}
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/8.Reusability and Scalability.png}
        \caption{Fairness vs Riusabilità e Scalabilità}
    \end{figure}
    
    In maniera più semplificata (aggregando i concetti), è stato chiesto ai partecipanti di provare a fornire un parere similare alle altre specifiche non funzionali quali manutenibilità \& retraining - figura 5.17 (in ottica evolutiva di un generico tool ML-Intensive) e scalabilità \& riusabilità - figura 5.18. Per questa tipologia di requisiti, i report grafici, possono facilmente far dedurre considerazioni similari rispetto al modello Furps+, ma probabilmente per arrivare ad osservazioni più precise, si dovrebbe analizzare separatamente i singoli dati (scelta non adottata per evitare di rendere troppo complesso il quesito ai partecipanti).\\\\
    
    In generale, le problematiche discriminatorie ed il concetto di software fairness, sono probabilmente concetti molto distanti dagli aspetti di qualità, già ampiamente studiati e sistematizzati. Allo stato della pratica, il generico lavoratore, sia esso data scientist, ingegnere del software, manager o quant'altro, difficilmente darà  più rilevanza in termini assoluti a fairness rispetto ad altre prerogative non funzionali (a maggior ragione per aspetti critici, quali accuracy, performance o sicurezza), e le cause possono essere molteplici, ad esempio:
    
    \begin{itemize}
        \item Probabilmente la rilevanza del concetto di fairness aumenterà mano a mano che la società percepirà le problematiche connesse come un problema prioritario, quindi tali confronti dovranno essere ripetuti in futuro;
        \item Fairness probabilmente necessita di essere analizzata in maniera diversa rispetto gli aspetti non funzionali standard, probabilmente in ragione alla specificità del dominio;

    \end{itemize}
    
    
    Avendo quindi osservato, come gli altri aspetti non funzionali, tendano ad essere considerabili da lievemente più rilevanti a nettamente più rilevanti rispetto la software fairness, considerando l'intero campione di analisi, è possibile provare a formalizzare qualche considerazione ulteriore, analizzando le risposte medie ottenute per singolo settore.\\\\ 
    \emph{N.B: Il singolo valore di confronto tra fairness ed un altro generico NFR, è stato calcolato come media matematica delle singole risposte dello specifico settore}, ad esempio se nella figura 5.19, per il settore Healthcare, è riportato valore 0 della scala quantitativa tra il confronto tra Fairness e Usability, è necessario considerare che \emph{mediamente} per il settore specifico l'aspetto non funzionale Usability è egualmente importante rispetto l'aspetto di Fairness nell'utilizzo/sviluppo di una soluzione ml-intensive.
    

    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/TradeOffsBySectors/1. UsabilityVsFairness.png}
        \caption{Fairness vs Usabilità per settore professionale}
    \end{figure}
    
    L'analisi di dettaglio per settore è disponibile online al link: \url{https://github.com/CFerrara98/Empirical-Investigation-On-Fairness-Development}. Per porre qualche considerazione ulteriore, sono state riportati: il diagramma inerente il confronto tra Usabilità e Fairness - figura 5.19 (rappresentativo della tendenza di risposta che hanno assunto i confronti specifici tra fairness e aspetti qualitativi più comuni tra i moduli Ml Intensive e altri sistemi IT,  e il grafico di confronto per settore tra Sicurezza e Fairness - figura 5.20, rappresentativo della tendenza di confronto assunta per aspetti non funzionali più tecnici in un modulo ML-Intensive.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ3/TradeOffsBySectors/6.SecurityAndSafetyVsFairness.png}
        \caption{Fairness vs Sicurezza per settore professionale}
    \end{figure}
    
    Analizzando quindi i \emph{trade-off} non funzionali raggruppando le risposte per settore, si evidenzia quindi come sia confermata la tendenza generale, ovvero che aspetti non funzionali più standardizzati tendono ad essere considerati dalla maggioranza del campione da lievemente più rilevanti ad estremamente più rilevanti, specialmente quando si parla di requisiti criticamente importanti per il buon funzionamento di un modulo ml-intensive e.g. l'aspetto di Sicurezza. Aumentando la granularità del confronto di rilevanza tra fairness e l'altro NFR considerato a livello settoriale, è però possibile porre qualche considerazione più rilevante, è il caso del confronto tra Sicurezza e Fairness, dove si evidenzia una criticità estremamente rilevante della prima rispetto la seconda per settori in cui lavorare in maniera priva di attacchi esterni è estremamente critico o addirittura vitale, e.g. il settore Governance, mentre lo stesso non può dirsi per settori dove gli aspetti di Sicurezza diventano estremamente meno rilevanti rispetto le discriminazioni dell'individuo, è il caso del settore dell'intrattenimento, dove Fairness è considerata addirittura lievemente più rilevante rispetto gli aspetti qualitativi di sicurezza, quasi in contro tendenza con gli altri settori specifici di analisi. \\\\
    
    Ovviamente la tendenza generale dei risultati per questo specifico goal di ricerca resta la stessa rispetto quella osservata a livello generale, ma porre l'accento su esempi di questo tipo, fornisce senz'altro una visione alternativa che fa intendere come fairness possa essere considerato un aspetto prioritario o meno rispetto altri aspetti qualitativi a seconda dello specifico dominio di utilizzo del modulo di machine learning in sviluppo.
    
    \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=Gray]
    			\begin{minipage}{\textwidth}
    				\textit{\faKey  \textbf{ Risultato di ricerca 3}}\\
    			L'etica di un modulo ML-Intensive, è un aspetto poco maturo secondo i professionisti se paragonato ad altre specifiche non funzionali più standardizzate, tale considerazione diventa particolarmente vera e più accentuata quando si analizzano specifiche più tecniche quali Sicurezza o Accuracy. Va però osservato che questi confronti hanno una variabilità elevata a seconda dello specifico dominio applicativo.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
	
	
    \subsection{Fairness come aspetto intrinseco di una pipeline ML}

    	\begin{center}
    	\hspace*{-5mm}\begin{tikzpicture}
    		\node [mybox] (box){%
    			\begin{minipage}{.70\textwidth}
    				\centering
    				\textit{RQ4 - In quali fasi di una tipica pipeline di Machine Learning è importante adottare strategie per garantire alti livelli di fairness?}
    			\end{minipage}
    		};
    	\end{tikzpicture}%
    \end{center}
    
     \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ4/Fairness strategies during a ML Pipeline development.png}
        \caption{Applicabilità di strategie di Fairness improoving in una Pipeline di Machine learining}
    \end{figure}
    Come già osservato in fase di progettazione, questo specifico sub goal, mira a valutare l'utilità di adottare strategie di fairness level improoving in ogni fase di una generica pipeline di machine learning. Dai dati e dal report grafico di figura 5.21, è  evidenziabile, come la \textbf{fairness di un sistema di machine learning è un aspetto di rilevanza in ogni fase di una generica pipeline}, ma da un occhiata più specifica, è facile dedurne come essa stessa può essere un aspetto qualitativo che migliora mano a mano che il sistema evolve sia dopo la fase di building, \textbf{dato che la fase di verifica e validazione viene considerata una delle più valide} per attestare il livello di fairness del sistema, \textbf{seguita dalla fase di analisi e statistica} (successiva al deploy del modello), e di conseguenza dalla \textbf{fase di preparazione dei dati}, che ovviamente va raffinata ad ogni ciclo di sviluppo di una tipica pipeline ML. Ciò significa che la pratica lavorativa, secondo i dati raccolti, probabilmente suggerisce che attualmente la Fairness è un aspetto di un modello di machine learning che va di pari passo con la sua evoluzione, ed è probabilmente lì che è necessario investire con soluzioni specifiche.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.9\textwidth]{figure/Analisi/RQ4/Fair ML Tools Development Usage.png}
        \caption{Utilizzo di tool noti per il Fair ML Development}
    \end{figure}
    
    Per fornire altri dettagli circa l'utilizzo di approcci e strumenti utili al trattamento di fairness come aspetto integrante del ciclo di sviluppo di un modulo ml, si osserva (figura 5.22) letteralmente pochi partecipanti all'indagine abbiano fatto dichiarato di non fare uso di tool specifici per il fairness improoving. la maggior parte di essi invece dichiara di aver utilizzato almeno una volta un tool proprietario tra i noti Microsoft Fairlearn, Google's-What-IF o IBM's AI Fairness 360. Ciò è sicuramente un buon segnale, dato che le aziende evidentemente, sono molto propense ad adottare strategie specifiche per progettare soluzioni ML Fair su larga scala.
    
    \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=Gray]
    			\begin{minipage}{\textwidth}
    				\textit{\faKey  \textbf{ Risultato di ricerca 4}}\\
    		     Trattare fairness è un qualcosa di rilevante in ogni fase di sviluppo ML Pipeline-Orineted. In particolare si fa notare la rilevanza di applicare strategie specifiche in fasi evolutive come Data Preparation, Verifica e Validazione o Statistica ed Analisi dei dati. Particolarmente di rilievo risulta essere anche l'utilizzo di tool o soluzioni specifiche dei principali vendor, quali Google o Microsoft. 
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
	
    \subsection{Fairness e maturità aziendale}
    
    \begin{center}
    	\hspace*{-5mm}\begin{tikzpicture}
    		\node [mybox] (box){%
    			\begin{minipage}{.70\textwidth}
    				\centering
    				\textit{RQ5 - Quanto le compagnie di sviluppo ML-Intensive, sono mature nel trattare il concetto di fairness come un requisito non funzionale?}
    			\end{minipage}
    		};
    	\end{tikzpicture}%
    \end{center}
    
    Ultimo punto conclusivo della panoramica analitica sullo stato della pratica lavorativa, è appunto cercare di capire se e come le aziende dei partecipanti all'indagine definiscono la problematica di fairness durante un generico progetto di sviluppo ML-Intensive e soprattutto quanto sono mature le politiche aziendali a riguardo. 
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ5/Fairness Treatment Maturity for companies.png}
        \caption{Risultati analitici del Fair Capability Maturity Model}
    \end{figure}
    
    Con la figura 5.23, è facile fornire una risposta preliminare a questo questo di ricerca, il contesto attuale, provando a generalizzare quanto osservabile sulla base del campione, è costituito da circa un \textbf{50\% delle aziende che praticano sviluppo di soluzioni ML-Intensive}, che tratta fairness ai \textbf{livelli 1 e 2}, ovvero, secondo la scala prefissata, trattano le problematiche connesse alla fairness in maniera sporadica o abitudinaria, ma senza l'utilizzo di standard specifici atti all'ottimizzazione. Interessante anche come dell'altra metà dei partecipanti, soltanto \textbf{l'11,2\% dichiari che la sua azienda non tratti affatto fairness}, infatti \textbf{la fetta restante di partecipanti colloca la propria azienda ai livelli 3, 4 e 5}, quindi dal trattare Fairness in maniera abitudinaria con specifici standard di sviluppo, fino all'applicazione di tecniche di process improoving sugli specifici processi aziendali. 
    
     \begin{figure}[h!]
        \centering
        \includegraphics[width=1\textwidth]{figure/Analisi/RQ5/Fairness treatment maturity model by country.png}
        \caption{Risultati analitici del Fair Capability Maturity Model per continente}
    \end{figure}
    
    Spostando l'analisi su un ottica più mirata, come quella continentale - figura 5.24, si nota ancora meglio come ad esempio in Europa (la maggioranza del campione) le aziende pratichino processi atti alla gestione della fairness. Ciò è senz'altro un fattore ampiamente positivo, dato che è facilmente osservabile come le aziende, ad esempio su scala europea, siano disposte a migliorare e ad applicare processi ingegneristici nell'ambito ml intensive, di pari passo all'evoluzione metodologica della ricerca e degli investimenti pubblici \cite{ritson201317}.
    
    \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=Gray]
    			\begin{minipage}{\textwidth}
    				\textit{\faKey  \textbf{ Risultato di ricerca 5}}\\
    		    Ad oggi, si osserva come il trattamento di fairness sia ancora poco maturo e standardizzato nelle pratiche aziendali ma questo aspetto è da considerarsi in costante miglioramento. Infatti nonostante metà dei partecipanti dichiari che la sua azienda tratti fairness in maniera poco sistematica, una buona parte del campione afferma che la propria realtà lavorativa tratti le specifiche etiche con standard ben definiti e/o volti all'ottimizzazione.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}

    
    
    \section{Discussioni e Implicazioni}
    
    \subsection{Quanto fairness è \emph{matura} nello sviluppo ML aziendale?}
    Partendo dall'analisi dello stato dell'arte, in questo lavoro di tesi si è osservato come effettivamente i ricercatori dell'ambito ingegneristico, evidenzino la necessità di trattare il concetto di software fairness come un vero e proprio aspetto qualitativo \emph{primario} nello sviluppo ML intensive, affinché le elaborazioni dei moduli prodotti e i risultati di predizione degli stessi, siano immuni dal presentare problematiche connesse a vulnerabilità discriminatorie \cite{brun2018software}. Sulla base dei dati raccolti dall'indagine, è senz'altro interessante notare, come la maggior parte delle compagnie coinvolte riconosca l'esistenza di questa problematica ed applichi strategie a riguardo, infatti, come osservabile  dai risultati del quinto quesito di ricerca, solo una minima parte dei partecipanti dichiara che la propria compagnia non tratta affatto Fairness (11,2 \%) durante i quotidiani processi di sviluppo, mentre, dal lato opposto, oltre il 30 \% dei partecipanti dichiara di 
    applicare Fairness con i dovuti standard propri dell'azienda di appartenenza fino all'applicare strategie specifiche di process improoving nel trattamento di fairness. A conferma di ciò, l'analisi dei dati permette di osservare come tool commerciali per il trattamento di Fairness in un modulo di machine learning esistano e risultano essere estremamente utili nello sviluppo di soluzioni eticamente corrette, in particolare i tools più quotati dagli intervistati sono  Microsoft Fairnlearn, che approccia lo sviluppo fair-critical con casi d'uso reali, e Google What If? che permette proprio di analizzare in maniera qualitativa e quantitativa i livelli di fairness di uno specifico modulo.\\
    
    
   \subsection{Quanto fairness è \emph{immatura} nello sviluppo ML aziendale?}
   Concentrandosi invece sui dati rimanenti del quinto punto di ricerca analizzato, va osservato come il 50 \% del campione di analisi, dichiara di trovarsi ai livelli 1 e 2 della scala di maturità formalizzata, quindi è opportuno chiedersi se effettivamente fairness è vista o meno a livello pratico come un aspetto prioritario rispetto altre specifiche non funzionali. Dall'analisi generale dei confronti tra fairness con altri aspetti di qualità in un modulo ML-Intensive si è visto effettivamente come le opinioni raccolte tendano ad essere in ogni caso tendenti al considerare Fairness da lievemente meno rilevante, come per i confronti con aspetti qualitativi appartenenti al modello FURPS+, ad estremamente meno rilevante, come nel caso di specifiche estremamente più tecniche e standardizzate quali Security o Performance del modello. Vero che dall'analisi settoriale è riscontrabile come questa tendenza sia estremamente differente a seconda del dominio specifico, e.g. la differente rilevanza media di Fairness rispetto a Security nei settori di Governance ed Intrattenimento, ma dai dati a disposizione è senz'altro evidente come l'aspetto etico sia ancora poco maturo e standardizzato per essere considerato una vera e propria specifica non funzionale in un modulo di machine learning rispetto a specifiche \emph{storiche} più documentate e standardizzate. Molto probabilmente a tal proposito la chiave di svolta è racchiusa nel supporto che la comunità scientifica può apportare allo stato della pratica, senz'altro incentrando i nuovi studi sulle necessità e percezioni che l'attuale stato della pratica evidenzia, ma anche verificando in maniera sistematica, se quanto già stato fatto fin ora possa essere applicato attivamente in contesti di sviluppo reali.
   
     \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=perla]
    			\begin{minipage}{\textwidth}
    				\textit{\faCaretSquareORight  \textbf{ Implicazione 1}}\\
    		     Nonostante Fairness sia percepita come una specifica qualitativa di rilevanza dai professionisti, sono molte le aziende che la trattano in maniera poco sistematica o con priorità minore rispetto altre specifiche di qualità più standardizzate. Al fine di invertire definitivamente questa tendenza, nuovi studi di ricerca sono sicuramente necessari e possono fare la differenza.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
    
    \subsection{Definire e misurare fairness in un caso reale di sviluppo ML}
    Come osservato dalla letteratura, molti sono le definizioni e le metriche formalizzate al fine di trattare fairness durante lo sviluppo ML-Intensive \cite{FairnessDefinitionExplained}. Dai risultati del primo quesito di ricerca, si può notare come gli esperti tendano ad apprezzare in larga misura tutti i gruppi di definizioni proposte, però se si cerca qualche dettaglio in più, è facile osservare come approcci formali basati su similarità matematica, siano leggermente preferiti rispetto ai più formali aspetti basati su puro calcolo probabilistico o su relazioni di dipendenze causali. D'altro canto però, osservando i risultati inerenti il quesito inerente l'applicazione di approcci pratici nel trattamento di fairness, gli stessi intervistati dichiarano di trovarsi più a proprio agio a ragionare intuitivamente sulle dipendenze causali tra feature sensibili e risultati, piuttosto che su aspetti più tecnici, quali possibili attività di tuning dei parametri fair-oriented. A prima vista ciò potrebbe essere quasi un contro senso, ma effettivamente quanto le attuali definizioni basate su dipendenze causali, sono intuibili dai lavoratori se considerate nella loro forma più formale? quanto i differenti gruppi di definizioni attualmente conosciuti sono applicabili in maniera generale tenendo conto delle svariate esigenze dei domini d'utilizzo? Intersecare ciò che i risultati di analisi ottenuti, si evince anche praticamente l'applicabilità dell'una o l'altra definizione di fairness, oppure l'utilizzo di una o più approcci pratici, dipenda in se per se dal dominio di utilizzo, ciò non solo per l'enorme variabilità di rilevanza riscontrata analizzando a livello settoriale i dati relativi al terzo quesito di ricerca, ma anche perché molti partecipanti facciano presente in maniera esplicita, che oltre agli approcci noti in letteratura, essi stessi facciano uso di tecniche diverse da quelle proposte, ad esempio l'applicazione di strategie empiriche o di indicatori statistici come osservato dall'analisi dei dati. Quindi dove la ricerca può intervenire in tal senso? In definitiva non è probabile che l'una o l'altra strategia sia univocamente condivisibile o applicabile data l'estrema specificità della tematica, ma da un analisi più specifica degli standard di sviluppo effettivamente sarà possibile individuare quali processi formali possano essere o meno applicabili a seconda delle più svariate esigenze di dominio.
    
      \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=perla]
    			\begin{minipage}{\textwidth}
    				\textit{\faCaretSquareORight  \textbf{ Implicazione 2}}\\
    		     Dall'analisi effettuata è principalmente emerso che l'ampia varietà di metodologie e approcci per il trattamento di fairness e la variabile rilevanza delle specifiche etiche rispetto al dominio di utilizzo, sono fattori da cui non si può prescindere. Detto ciò è facilmente intuibile come la specificità del problema è una chiave di lettura essenziale per i futuri studi di ricerca.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
    
    \subsection{Processi di sviluppo ML Fair-Oriented, una visione a lungo termine}
    
    Constatato che fairness secondo l'attuale stato della pratica continua ad essere un aspetto estremamente dipendete dal dominio applicativo, e che gli studi a riguardo necessitino ancora di essere espansi tenendo conto in primo luogo dei bisogni pratici evidenziati dai professionisti, resta doveroso cercare di capire dove effettivamente la ricerca possa intervenire affinché il trattamento di fairness possa essere sistematizzato al pari di altre specifiche qualitative di un modulo ML-Intensive. È oramai noto alla comunità scientifica, ma anche allo stato della pratica che approcci di sviluppo evolutivo quali il noto standard MLOps, siano una strategia vincente nel trattare e migliorare le specifiche di qualità di un modulo intelligente, soprattutto nel contesto di MLOps, sono nate strategie specifiche che consentono di monitorare i livelli di un particolare aspetto non funzionale, ma ciò è possibile anche nel caso dell'etica? dall'analisi dei dati ricevuti in risposta ai quesiti inerenti le fasi di una canonica pipeline di machine learning e delle figure di rilievo in un team orientato allo sviluppo fair-oriented probabilmente ciò è assolutamente un buon punto su cui riflettere per futuri lavori di ricerca. \\
    
    Partendo proprio dall'analisi delle fasi di una canonica pipeline di machine learning, si è visto come gli esperti del settore propongano come più critico il trattamento di fairness nelle fasi di gestione e manipolazione dei dati di training, validazione del modello oppure nelle statistiche di monitoraggio del modello in esecuzione nel contesto d'utilizzo. Proprio in questo contesto la ricerca offre già numerose soluzioni, basti pensare alle tecniche specifiche di selezione dei dati basate su diversità statistica \cite{moumoulidou2020diverse} oppure alle specifiche tecniche di testing fair-orinted e ri-bilanciamento del campione \cite{galhotra2017fairness}. Ma quanto queste strategie specifiche, sono conosciute ed applicate dai professionisti? Quanto invece nuove strategie come nuovi modelli di validazione fair-specific sono necessarie per sistematizzare questi processi? tutto questo, quasi sicuramente necessita di ulteriori approfondimenti, e nuovi lavori quali l'analisi di bad \& best practices specifiche per il trattamento di fairness in un modulo ML-Intensive o la ricerca di maggiori cause di discriminazione partendo ad esempio dall'analisi delle principali feature sensibili comuni alla maggior parte dei datasets di addestramento, possono essere senz'altro un buon punto di partenza.
    
    \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=perla]
    			\begin{minipage}{\textwidth}
    				\textit{\faCaretSquareORight  \textbf{ Implicazione 3}}\\
    		    Ricercare nuove soluzioni per lo sviluppo ML Fair-Oriented o sistematizzare quelle già esistenti, implica un'ottica di analisi che tenga conto della natura evolutiva delle specifiche etiche. Fasi come Data Preparation o Validazione del modello si dimostrano essere fasi cruciali nell'evoluzione di un modulo ML.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
    
    In diretta connessione all'applicabilità di strategie, in un qualsiasi processo di sviluppo che si rispetti, va senz'altro definito chi debba assumere l'una o l'altra responsabilità. Dai dati ottenuti si evidenziano due principali osservazioni:
    \begin{itemize}
        \item  Ingegneri del software e data scientist restano a pari merito figure cruciali e di simili rilevanza anche nel trattamento di specifiche problematiche quali i livelli di fairness nei moduli;
        \item La definizione di linee guida di management o il coinvolgimento di esperti specifici per il trattamento di software fairness sono aspetti che maggiormente fanno la differenza.
    \end{itemize}
    
    Ma da queste osservazioni possono anche riscontrarsi nuovi quesiti da porsi ed analizzare, ad esempio:
    
    \begin{itemize}
        \item Quando risulta essere cruciale che ruoli diversi quali data scientist e ingegneri del software lavorino in sinergia se si considera una pipeline di machine learning?
        \item Quali possono essere i punti di partenza per la definizione di standard di management specifici?
        \item Cosa si intende effettivamente per esperti nell'ambito Fairness? Esperti di socio-culturali di etica? Esperti del dominio specifico o quant'altro?
    \end{itemize}
    
    \begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=perla]
    			\begin{minipage}{\textwidth}
    				\textit{\faCaretSquareORight  \textbf{ Implicazione 4}}\\
    		   Nella definizione di strategie fair-oriented legate allo sviluppo evolutivo dei sistemi ML-Intensive, vanno anche approfonditi aspetti specifici legati alle responsabilità delle singole figure professionali, alcuni punti di studio intuibili dall'indagine sono infatti: la necessità di formalizzare le responsabilità dei manager nel trattamento di fairness, l'overlap di responsabilità tra ingegneri del software e data scientist anche in questo specifico caso, oppure il definire formalmente quali esperti coinvolgere a seconda del problema specifico.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
    Oltre ciò, anche altri dati possono essere di ispirazione per nuovi studi, non ultimo la minore rilevanza data dai partecipanti ad altri aspetti critici di una pipeline di machine learning come l'ingegnerizzazione dei requisiti o la preparazione del modello. In generale il principale contributo che si evince da questo lavoro di tesi sta proprio nella consapevolezza che l'etica di un modulo di machine learning necessita di maturare ed evolvere in maniera sistematica proprio perché le cause e i fattori che impattano tale tematica oggigiorno sono sempre più evidenti e dai risultati ottenuti è senz'altro intuibile dove sia più utile intervenire in funzione di questi obiettivi.
    
	
	 
	
	
	
	\begin{center}
	
        \begin{tcolorbox}[width=\textwidth, colframe=black, colback=perla]
    			\begin{minipage}{\textwidth}
    				\textit{\faCaretSquareORight  \textbf{ Implicazione 5}}\\
    		    Lavorare con fairness significa anche capire effettivamente come fairness si caratterizza nel dominio specifico, quindi risulta particolarmente importante anche adottare nuove strategie che permettano di mappare definizioni ed approcci sulla base delle specifiche del dominio del problema. In tal senso, sarà senz'altro necessario indagare anche su nuove metodologie didattiche che approccino la tematica tenendo conto di questa specifica esigenza.
    			\end{minipage}
		\end{tcolorbox}
	\end{center}
\newpage
